{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88231094-385b-45cb-933c-232c36c08582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, ICRS, Galactic, FK4, FK5\n",
    "from swifttools.swift_too import Data, ObsQuery, TOORequests\n",
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "class UVOTZ():\n",
    "\n",
    "    def __init__(self, analysis_root_dir, src_catalog_dir = None):\n",
    "        \"\"\"Defines the initial parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        analysis_root_path: str; the directory where the data will be downloaded and analyzed\n",
    "        src_catalog_dir: str; the csv file that stores the source name, coordiantes and time window.\n",
    "        \"\"\"\n",
    "\n",
    "        self.analysis_root_dir = analysis_root_dir\n",
    "        self.full_filter_list = [\"ubb\", \"um2\", \"uuu\", \"uvv\", \"uw1\", \"uw2\"]\n",
    "        self.src_catalog_dir = src_catalog_dir\n",
    "\n",
    "        if not self.src_catalog_dir:\n",
    "            print(\"You haven't defined the src meta info yet!\")\n",
    "            df = pd.DataFrame(columns=[\"name\", \"ra\", \"dec\", \"window_lower\", \"window_upper\"])\n",
    "            df.to_csv(self.analysis_root_dir+\"/metadata.csv\", sep = \",\", index=False)\n",
    "            print(f\"A template csv file is generated at {self.analysis_root_dir}!\")\n",
    "\n",
    "        self.data_dir = self.analysis_root_dir + \"/data\"\n",
    "        _ = self.create_folder(self.data_dir)  # here I don't need the returned value\n",
    "\n",
    "    def str_to_list(self, str):\n",
    "        return str[1:-2].replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "    def find_keywords(self, file, *keywords):\n",
    "        \"\"\"\n",
    "        Find the value of the keywords from a fits file. It deals with multi-extension fits files\n",
    "        so you will get a dictionary of all the requested keywords in each extension.\n",
    "        \"ext_No\" and \"EXTNAME\" are mandatory in order to tag the origin of the keyword values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file: str; the directory to the fits file\n",
    "        kewords: str(s); the keywords you want to query\n",
    "\n",
    "        Example run\n",
    "        -----------\n",
    "        find_keywords(file_path, \"ASPCORR\", \"HDUCLAS1\")\n",
    "        Outï¼š\n",
    "        {'ext_No': [1, 2],\n",
    "        'EXTNAME': ['bb649482961I', 'bb649505811I'],\n",
    "        'ASPCORR': ['DIRECT', 'DIRECT'],\n",
    "        'HDUCLAS1': ['IMAGE', 'IMAGE']}\n",
    "        \"\"\"\n",
    "\n",
    "        with fits.open(file) as hdul:\n",
    "            ext_nums = len(hdul) - 1\n",
    "\n",
    "            # initialize the dictionary\n",
    "            dict_ext = {}\n",
    "            dict_ext[\"ext_No\"] = [i for i in np.arange(1, ext_nums+1)]\n",
    "            dict_ext[\"EXTNAME\"] = [None] * ext_nums  # [None, None, None, ...]\n",
    "            for i in keywords:\n",
    "                dict_ext[i] = [None] * ext_nums  # generate the keys for the keywords like ext_No and EXTNAME\n",
    "\n",
    "            \n",
    "            # read and record the keywords in the dictionary\n",
    "            n_ = 1\n",
    "            while n_ <= ext_nums:\n",
    "                dict_ext[\"EXTNAME\"][n_-1] = hdul[n_].header[\"EXTNAME\"]\n",
    "                for i in keywords:\n",
    "                    dict_ext[i][n_-1] = hdul[n_].header[i]\n",
    "                n_ += 1\n",
    "\n",
    "        return dict_ext\n",
    "\n",
    "    def create_folder(self, folder_dir):\n",
    "        \"\"\"\n",
    "        Check if the folder exists and create the folder if not.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_path : str; the path of the folder you want to create.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        path_exist : boolean; False if the folder_path doesn't exist;\n",
    "                     True if the folder_path already exist.\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isdir(folder_dir) != True:\n",
    "            os.mkdir(folder_dir)\n",
    "            path_exist = False\n",
    "        else:\n",
    "            path_exist = True\n",
    "            \n",
    "        return path_exist\n",
    "\n",
    "    def del_then_create_folder(self, folder_dir):\n",
    "        \"\"\"\n",
    "        Delete folder if exists and then create the folder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_dir: str; the directory of the folder\n",
    "        \"\"\"\n",
    "        \n",
    "        if os.path.exists(folder_dir):\n",
    "            shutil.rmtree(folder_dir)\n",
    "        os.mkdir(folder_dir)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def get_obsquery_input(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate the input for Swiftttools.\n",
    "        swifttools accept various types of inputs including obsid, targetid and ra&dec.\n",
    "        Here the most used one is ra&dec, which is also the default input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "\n",
    "        if \"obsid\" in kwargs.keys():\n",
    "            return {\"name\":kwargs[\"name\"],\n",
    "                    \"obsid\": kwargs[\"obsid\"]}\n",
    "            \n",
    "        elif \"targetid\" in kwargs.keys():\n",
    "            return {\"name\":kwargs[\"Name\"],\n",
    "                    \"targetid\": kwargs[\"targetid\"]}\n",
    "\n",
    "        elif \"ra\" and \"dec\" in kwargs.keys():\n",
    "            ra_dec = {key: value for key, value in kwargs.items() if key in {\"ra\", \"dec\"}}\n",
    "            skycoord = SkyCoord(**ra_dec, unit = (u.hourangle, u.deg), frame = \"icrs\")\n",
    "            \n",
    "            return {\"name\":kwargs[\"name\"],\n",
    "                    \"skycoord\": skycoord}\n",
    "            \n",
    "    def download_swift_data(self, radius = 5/60, uvotmode = \"0x30ed\"):\n",
    "\n",
    "        # read the csv file\n",
    "        df = pd.read_csv(self.src_catalog_dir, sep = \",\")\n",
    "        row_nums = df.shape[0]\n",
    "\n",
    "        obsid = []  # the obsid for all the sources\n",
    "        obsid_saving_dir = []  # the obsid path for all the sources\n",
    "        obsid_time = []  # the obsid time for all the sources\n",
    "        files = []\n",
    "        filters = []\n",
    "\n",
    "        for i in np.arange(row_nums):\n",
    "\n",
    "            info_ = dict(df.iloc[i])  # convert each row into a dict that contains the src info\n",
    "            src_name = info_[\"name\"].replace(\" \", \"_\")\n",
    "            src_ra = info_[\"ra\"]\n",
    "            src_dec = info_[\"dec\"]\n",
    "            src_window_lower = info_[\"window_lower\"]\n",
    "            src_window_upper = info_[\"window_upper\"]\n",
    "            self.src_dir = self.data_dir + f\"/{src_name}\"\n",
    "            self.del_then_create_folder(self.src_dir)\n",
    "\n",
    "            target_info = self.get_obsquery_input(**info_)\n",
    "            print(target_info)\n",
    "            oq = ObsQuery(radius = radius, begin = src_window_lower, end = src_window_upper, **target_info)\n",
    "            id_ = \"00000000000\"  # this variable will be used to avoid downloading the same data file mutiple times by comparing the observation id\n",
    "\n",
    "            src_obsid = []\n",
    "            src_saving_dir = []\n",
    "            src_obsid_time = []\n",
    "            for i in np.flip(np.arange(-len(oq), 0)):\n",
    "                if oq[i].obsid == id_:\n",
    "                    print(f\"{oq[i].obsid} has been downloaded/examined, skipping ...\")\n",
    "                else:\n",
    "                    if oq[i].uvot_mode == uvotmode:\n",
    "                        date_ = oq[i].begin.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"{oq[i].obsid} on {date_} is being downloaded, the uvot mode is {oq[i].uvot_mode}\")\n",
    "                        oq[i].download(uvot = True, outdir = self.src_dir)\n",
    "                        src_obsid.append(oq[i].obsid)  # record the obsid for this source\n",
    "                        src_saving_dir.append(self.src_dir + \"/\" + oq[i].obsid)  # record the path to obsids for this source\n",
    "                        src_obsid_time.append(date_)\n",
    "                        id_ = oq[i].obsid\n",
    "                    else:\n",
    "                        print(f\"{oq[i].obsid} uvod mode {oq[i].uvot_mode} is not the one you requested as {uvotmode}, skipping ...\")\n",
    "                        id_ = oq[i].obsid\n",
    "                    \n",
    "            obsid.append(src_obsid)\n",
    "            obsid_saving_dir.append(src_saving_dir)\n",
    "            obsid_time.append(src_obsid_time)\n",
    "            print(f\"Source {src_name} download finished!\")\n",
    "            print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "        self.meta_data_dir = self.data_dir + \"/metadata.csv\"\n",
    "        df[\"obsid\"] = obsid\n",
    "        df[\"obsid_saving_dir\"] = obsid_saving_dir\n",
    "        df[\"obsid_time\"] = obsid_time\n",
    "        df.to_csv(self.meta_data_dir, sep = \",\", index=False)\n",
    "\n",
    "        return\n",
    "\n",
    "    def check_ASPCORR(self, file):\n",
    "        \"\"\"\n",
    "        Check the ASPCORR keywords.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file: str; the fits file to be exmined.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        Boolean\n",
    "        \"\"\"\n",
    "        \n",
    "        dict_exts = self.find_keywords(file, \"ASPCORR\")\n",
    "        asps = dict_exts[\"ASPCORR\"]\n",
    "        check = [\"Yes\" for i in asps if \"DIRECT\" in i]\n",
    "        if len(check) == len(asps):\n",
    "            return True\n",
    "        elif len(check) != len(asps):\n",
    "            return False\n",
    "\n",
    "    def get_src_region(self, saving_dir, ra, dec):\n",
    "        \n",
    "        reg_files = [saving_dir + \"/\" + i + \".reg\" for i in self.full_filter_list]\n",
    "        \n",
    "        for reg in reg_files:\n",
    "            f = open(reg, \"w\")\n",
    "            f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "            f.write('global color=green dashlist=8 3 width=1 font=\"helvetica 10 normal roman\" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n')\n",
    "            f.write(\"fk5\\n\")\n",
    "            f.write(f'circle({ra},{dec},5.000\")')\n",
    "            f.close()\n",
    "            \n",
    "        return reg_files\n",
    "\n",
    "    def sum_images(self, file, sum_type, **kwargs):\n",
    "        \"\"\"\n",
    "        Sum the images from multiple extensions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file: str; the directory of the file\n",
    "        sum_type: str; the case of the sum\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_dir: str; the directory of the output file\n",
    "        \"\"\"\n",
    "        if sum_type == \"one_only\":  # when there is only one obs file\n",
    "            filter_name = file[-13:-10]  # get the filer name from the file name\n",
    "            out_dir = kwargs[\"summed_dir\"] + f\"/all{filter_name}.fits\"\n",
    "            os.system(f\"uvotimsum infile={file} outfile={out_dir} | tee -a uvotimsum_log.txt >/dev/null 2>&1\")\n",
    "\n",
    "        elif sum_type == \"intermediate\":  # when there are multiple obs files, sum each by each to get intermediate files\n",
    "            filter_name = file[-13:-10]\n",
    "            obsid = file[-24:-13]\n",
    "            out_dir = kwargs[\"summed_dir\"] + f\"/{filter_name}_{obsid}.fits\"\n",
    "            os.system(f\"uvotimsum infile={file} outfile={out_dir} | tee -a uvotimsum_log.txt >/dev/null 2>&1\")\n",
    "\n",
    "        elif sum_type == \"exclude=NONE\":  # when you sum the summed obs files\n",
    "            filter_name = file[-8:-5]\n",
    "            out_dir = kwargs[\"summed_dir\"] + f\"/all{filter_name}.fits\"\n",
    "            os.system(f\"uvotimsum exclude=NONE infile={file} outfile={out_dir} | tee -a uvotimsum_log.txt >/dev/null 2>&1\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Wrong sum type! Please use final or intermediate\")\n",
    "            #break\n",
    "        return out_dir\n",
    "        \n",
    "\n",
    "    def sum_obs_files(self, meta_data_dir = None):\n",
    "        \"\"\"\n",
    "        Sum the multiple obs files for the same source.\n",
    "        \"\"\"\n",
    "\n",
    "        if meta_data_dir == None:\n",
    "            df = pd.read_csv(self.meta_data_dir, sep = \",\")\n",
    "        else:\n",
    "            df = pd.read_csv(meta_data_dir, sep = \",\")\n",
    "        df[\"final_fits\"] = None\n",
    "        df[\"reg_files\"] = None\n",
    "\n",
    "        for i in np.arange(df.shape[0]):\n",
    "            final_fits = []\n",
    "            obsids = self.str_to_list(df.loc[i,\"obsid\"])\n",
    "            src_name =  df.loc[i, \"name\"].replace(\" \", \"_\")\n",
    "            ra = df.loc[i,\"ra\"]\n",
    "            dec = df.loc[i,\"dec\"]\n",
    "            print(f\"Now summing {df.iloc[i,0]}\")\n",
    "\n",
    "            # create the dir to save the summed images\n",
    "            summed_dir = self.data_dir + f\"/{src_name}\" + \"/Summed\"\n",
    "            self.create_folder(summed_dir)\n",
    "\n",
    "            if len(obsids) == 1:  # it means there is only one sky image\n",
    "                obsid_dir = self.str_to_list(df.loc[i,\"obsid_saving_dir\"])[0]\n",
    "                img_files = glob.glob(obsid_dir + \"/uvot/image/\" + \"*sk.img.gz\")\n",
    "                for img_file in img_files:\n",
    "                    if self.check_ASPCORR(img_file):\n",
    "                        summed_fits = self.sum_images(img_file, sum_type = \"one_only\", summed_dir = summed_dir)\n",
    "                        final_fits.append(summed_fits)\n",
    "\n",
    "            else:\n",
    "                for obsid_dir in self.str_to_list(df.loc[i,\"obsid_saving_dir\"]):\n",
    "                    img_files = glob.glob(obsid_dir + \"/uvot/image/\" + \"*sk.img.gz\")\n",
    "                    for img_file in img_files:\n",
    "                        if self.check_ASPCORR(img_file):\n",
    "                            self.sum_images(img_file, sum_type = \"intermediate\", summed_dir = summed_dir)\n",
    "                            \n",
    "                intermediate_files = glob.glob(summed_dir + \"/*fits\")  # note fits are the summed sky images\n",
    "                for filter_ in self.full_filer_list:\n",
    "                    # find the fits files according to the filer name\n",
    "                    filter_fits = [fits for fits in intermediate_files if filter_ in fits]\n",
    "                    if len(filter_fits) == 1:\n",
    "                        shutil.copy2(filter_fits[0], filter_fits[0][0:-21] + f\"/all_{filter_}.fits\")\n",
    "\n",
    "                    else:\n",
    "                        fappended_file = filter_fits[-1][0:-21] + f\"/_{filter_}.fits\"  # the file to be appended on\n",
    "                        shutil.copy2(filter_fits[-1], fappended_file)  # make a copy\n",
    "\n",
    "                        for j in filter_fits[0:-1]:\n",
    "                            os.system(f\"fappend {j} {fappended_file}\")\n",
    "\n",
    "                        summed_fits = self.sum_images(fappended_file, sum_type=\"exclude=NONE\", summed_dir = summed_dir)\n",
    "                        final_fits.append(summed_fits)\n",
    "                        \n",
    "                _ = self.create_folder(summed_dir + \"/intermediate\")\n",
    "                _ = summed_dir + \"/*_*\"  # only the intermediate files contain _\n",
    "                inter_dir = summed_dir + \"/intermediate\"\n",
    "                os.system(f\"mv {_} {inter_dir}\")\n",
    "\n",
    "            reg_files = self.get_src_region(saving_dir = summed_dir, ra = ra, dec = dec)\n",
    "            \n",
    "            df.loc[i, \"final_fits\"] = str(final_fits)\n",
    "            df.loc[i,\"reg_files\"] = str(reg_files)\n",
    "\n",
    "        df.to_csv(self.data_dir+\"/metadata.csv\", sep = \",\", index=False)\n",
    "\n",
    "    def extract_mag(self, fits_file):\n",
    "        \"\"\"\n",
    "        Extract the AB magnitude from the photometry result file.\n",
    "        \"\"\"\n",
    "        hdul = fits.open(fits_file)\n",
    "        data = hdul[1].data\n",
    "\n",
    "        mag = np.round(data[\"AB_MAG\"][0], decimals=2)\n",
    "        stat_err = np.round(data[\"AB_MAG_ERR_STAT\"][0], decimals=2)\n",
    "        sys_err = np.round(data[\"AB_MAG_ERR_SYS\"][0], decimals=2)\n",
    "        err = round(np.sqrt(stat_err**2+sys_err**2),2)\n",
    "        \n",
    "\n",
    "        return mag, err\n",
    "\n",
    "    def extract_filter(self, fits_file):\n",
    "        \"\"\"\n",
    "        Extract the filter info from the photometry result file.\n",
    "        \"\"\"\n",
    "        hdul = fits.open(fits_file)\n",
    "        FILTER = hdul[1].header[\"FILTER\"]\n",
    "        if FILTER == \"B\":\n",
    "            filter = \"ubb\"\n",
    "        elif FILTER == \"UVM2\":\n",
    "            filter = \"um2\"\n",
    "        elif FILTER == \"U\":\n",
    "            filter = \"uuu\"\n",
    "        elif FILTER == \"V\":\n",
    "            filter = \"uvv\"\n",
    "        elif FILTER == \"UVW1\":\n",
    "            filter = \"uw1\"\n",
    "        elif FILTER == \"UVW2\":\n",
    "            filter = \"uw2\"\n",
    "        else:\n",
    "            print(\"The filter of the image isn't in the filter list!\")\n",
    "\n",
    "        return filter\n",
    "\n",
    "\n",
    "    def uvot_photometry(self, meta_data_dir = None):\n",
    "        \"\"\"\n",
    "        Perform photometry.\n",
    "        \"\"\"\n",
    "\n",
    "        if meta_data_dir == None:\n",
    "            df = pd.read_csv(self.meta_data_dir, sep = \",\")\n",
    "        else:\n",
    "            df = pd.read_csv(meta_data_dir, sep = \",\")\n",
    "\n",
    "        df_results = pd.DataFrame(columns = ['source_name', \"uw2\", \"uw2_err\", \"um2\", \"um2_err\", \"uw1\", \"uw1_err\", \"uuu\", \"uuu_err\", \"ubb\", \"ubb_err\", \"uvv\", \"uvv_err\"])\n",
    "\n",
    "        for i in np.arange(df.shape[0]):\n",
    "\n",
    "            src_name = df.loc[i, \"name\"].replace(\" \", \"_\")\n",
    "            \n",
    "            dict_new = {'source_name' : src_name,\n",
    "                        \"uw2\" : -99, \n",
    "                        \"uw2_err\" : -99,\n",
    "                        \"um2\" : -99,\n",
    "                        \"um2_err\" : -99,\n",
    "                        \"uw1\" : -99,\n",
    "                        \"uw1_err\" : -99,\n",
    "                        \"uuu\" : -99,\n",
    "                        \"uuu_err\" : -99,\n",
    "                        \"ubb\" : -99,\n",
    "                        \"ubb_err\" : -99,\n",
    "                        \"uvv\" : -99,\n",
    "                        \"uvv_err\" : -99}\n",
    "            df_results = df_results.append(dict_new,ignore_index = True)\n",
    "\n",
    "            for f in self.str_to_list(df.loc[0,\"final_fits\"]):\n",
    "\n",
    "                all_reg_files = self.str_to_list(df.loc[0,\"reg_files\"])\n",
    "\n",
    "                # defining all kinds of inputs and outputs also the column names\n",
    "                fits_file = f\n",
    "                filter_ = self.extract_filter(fits_file)\n",
    "                filter_err = filter_ + \"_err\"\n",
    "                src_region_file = [x for x in all_reg_files if filter_ in x][0]  # I did this to make sure the we use the correct region file for the fit image (selection based on the filter of the fits file)\n",
    "                print(src_region_file)\n",
    "                summed_dir = self.data_dir + f\"/{src_name}\" + \"/Summed\"\n",
    "                bkg_region_file = summed_dir + f\"/bg{filter_}.reg\"\n",
    "                outfits = summed_dir + f\"/{filter_}_Results.fits\"\n",
    "                outtxt = summed_dir + f\"/{filter_}_Results.txt\"\n",
    "\n",
    "                # run the command\n",
    "                print(f\"Running uvotsource image={fits_file} srcreg={src_region_file} bkgreg={bkg_region_file} sigma=3 cleanup=y clobber=y outfile={outfits} | tee {outtxt} >/dev/null\")\n",
    "                os.system(f\"uvotsource image={fits_file} srcreg={src_region_file} bkgreg={bkg_region_file} sigma=3 cleanup=y clobber=y outfile={outfits} | tee {outtxt} >/dev/null\")\n",
    "\n",
    "                # extract the magnitudes\n",
    "                ab_mag, ab_err = self.extract_mag(outfits)\n",
    "\n",
    "                print(filter_)\n",
    "                print(ab_mag)\n",
    "                print(ab_err)\n",
    "                print(\"++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "                index = df_results.shape[0]-1  # always append the data to the last row\n",
    "                df_results.loc[index, filter_] = ab_mag\n",
    "                df_results.loc[index, filter_err] = ab_err\n",
    "\n",
    "                df_results.to_csv(self.data_dir+\"/Magnitudes.csv\", sep = \",\", index=False)\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1410b58d-a7cc-4a76-a181-94abb5c68755",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HEADASNOQUERY\"]=\"\"\n",
    "os.environ['HEADASPROMPT'] = '/dev/null'\n",
    "analysis = UVOTZ(\".\", \"catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461e2c8d-6e06-4ad5-abb5-68b2c463d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'RX J1651.6+7218', 'skycoord': <SkyCoord (ICRS): (ra, dec) in deg\n",
      "    (252.91641667, 72.30690556)>}\n",
      "00015119001 on 2022-04-17 07:35:02 is being downloaded, the uvot mode is 0x30ed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [03:25<00:00,  4.57s/files]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00015119001 has been downloaded/examined, skipping ...\n",
      "Source RX_J1651.6+7218 download finished!\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analysis.download_swift_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69ef845c-03c4-4e73-801e-bef716086dbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UVOTZ' object has no attribute 'meta_data_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_obs_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 280\u001b[0m, in \u001b[0;36mUVOTZ.sum_obs_files\u001b[0;34m(self, meta_data_dir)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mSum the multiple obs files for the same source.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta_data_dir \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_data_dir\u001b[49m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(meta_data_dir, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UVOTZ' object has no attribute 'meta_data_dir'"
     ]
    }
   ],
   "source": [
    "analysis.sum_obs_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "300dad68-b22f-4b09-9823-ce39f101ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8656/3827565886.py:410: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/RX_J1651.6+7218/Summed/alluuu.fits\n",
      "uuu\n",
      "./data/RX_J1651.6+7218/Summed/uuu.reg\n",
      "Running uvotsource image=./data/RX_J1651.6+7218/Summed/alluuu.fits srcreg=./data/RX_J1651.6+7218/Summed/uuu.reg bkgreg=./data/RX_J1651.6+7218/Summed/bguuu.reg sigma=3 cleanup=y clobber=y outfile=./data/RX_J1651.6+7218/Summed/uuu_Results.fits | tee ./data/RX_J1651.6+7218/Summed/uuu_Results.txt >/dev/null\n",
      "uuu\n",
      "19.47\n",
      "0.14\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "./data/RX_J1651.6+7218/Summed/allubb.fits\n",
      "ubb\n",
      "./data/RX_J1651.6+7218/Summed/ubb.reg\n",
      "Running uvotsource image=./data/RX_J1651.6+7218/Summed/allubb.fits srcreg=./data/RX_J1651.6+7218/Summed/ubb.reg bkgreg=./data/RX_J1651.6+7218/Summed/bgubb.reg sigma=3 cleanup=y clobber=y outfile=./data/RX_J1651.6+7218/Summed/ubb_Results.fits | tee ./data/RX_J1651.6+7218/Summed/ubb_Results.txt >/dev/null\n",
      "ubb\n",
      "19.03\n",
      "0.16\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "./data/RX_J1651.6+7218/Summed/alluw2.fits\n",
      "uw2\n",
      "./data/RX_J1651.6+7218/Summed/uw2.reg\n",
      "Running uvotsource image=./data/RX_J1651.6+7218/Summed/alluw2.fits srcreg=./data/RX_J1651.6+7218/Summed/uw2.reg bkgreg=./data/RX_J1651.6+7218/Summed/bguw2.reg sigma=3 cleanup=y clobber=y outfile=./data/RX_J1651.6+7218/Summed/uw2_Results.fits | tee ./data/RX_J1651.6+7218/Summed/uw2_Results.txt >/dev/null\n",
      "uw2\n",
      "20.23\n",
      "0.1\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "./data/RX_J1651.6+7218/Summed/allum2.fits\n",
      "um2\n",
      "./data/RX_J1651.6+7218/Summed/um2.reg\n",
      "Running uvotsource image=./data/RX_J1651.6+7218/Summed/allum2.fits srcreg=./data/RX_J1651.6+7218/Summed/um2.reg bkgreg=./data/RX_J1651.6+7218/Summed/bgum2.reg sigma=3 cleanup=y clobber=y outfile=./data/RX_J1651.6+7218/Summed/um2_Results.fits | tee ./data/RX_J1651.6+7218/Summed/um2_Results.txt >/dev/null\n",
      "um2\n",
      "20.0\n",
      "0.13\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "./data/RX_J1651.6+7218/Summed/alluw1.fits\n",
      "uw1\n",
      "./data/RX_J1651.6+7218/Summed/uw1.reg\n",
      "Running uvotsource image=./data/RX_J1651.6+7218/Summed/alluw1.fits srcreg=./data/RX_J1651.6+7218/Summed/uw1.reg bkgreg=./data/RX_J1651.6+7218/Summed/bguw1.reg sigma=3 cleanup=y clobber=y outfile=./data/RX_J1651.6+7218/Summed/uw1_Results.fits | tee ./data/RX_J1651.6+7218/Summed/uw1_Results.txt >/dev/null\n",
      "uw1\n",
      "20.09\n",
      "0.15\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "./data/RX_J1651.6+7218/Summed/alluvv.fits\n",
      "uvv\n",
      "./data/RX_J1651.6+7218/Summed/uvv.reg\n",
      "Running uvotsource image=./data/RX_J1651.6+7218/Summed/alluvv.fits srcreg=./data/RX_J1651.6+7218/Summed/uvv.reg bkgreg=./data/RX_J1651.6+7218/Summed/bguvv.reg sigma=3 cleanup=y clobber=y outfile=./data/RX_J1651.6+7218/Summed/uvv_Results.fits | tee ./data/RX_J1651.6+7218/Summed/uvv_Results.txt >/dev/null\n",
      "uvv\n",
      "18.9\n",
      "0.29\n",
      "++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "analysis.uvot_photometry(\"./data/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814733b-ad16-406f-948e-848430754449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e95b8d-5125-4543-9b35-21285e6fb95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
